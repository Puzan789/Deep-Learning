{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-13T18:26:04.986356Z","iopub.execute_input":"2024-01-13T18:26:04.987162Z","iopub.status.idle":"2024-01-13T18:26:04.993147Z","shell.execute_reply.started":"2024-01-13T18:26:04.987127Z","shell.execute_reply":"2024-01-13T18:26:04.992139Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow\nimport requests\nfrom bs4 import BeautifulSoup","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:57:45.240178Z","iopub.execute_input":"2024-01-15T17:57:45.240568Z","iopub.status.idle":"2024-01-15T17:57:59.947422Z","shell.execute_reply.started":"2024-01-15T17:57:45.240532Z","shell.execute_reply":"2024-01-15T17:57:59.946268Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"response=requests.get('https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt')\nsoup=BeautifulSoup(response.content,'html.parser')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:58:05.224045Z","iopub.execute_input":"2024-01-15T17:58:05.225508Z","iopub.status.idle":"2024-01-15T17:58:05.688076Z","shell.execute_reply.started":"2024-01-15T17:58:05.225450Z","shell.execute_reply":"2024-01-15T17:58:05.687274Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"code","source":"import spacy\nimport re,string\n# from nltk.corpus import stopwords\n# nlp = spacy.load('en_core_web_sm')\n# nlp.max_length = 5281520\n\n# def preprocess(texts):\n#     doc = nlp(texts)\n#     text=\" \" .join([token.lemma_ for token in doc])\n#     text = re.sub(r'\\&\\w*;','',text)\n#     text = re.sub('@[^\\s]+','',text)   \n#     text = re.sub(r'https?:\\/\\/.*\\/\\w*','',text)\n#     text = re.sub(r'#\\w*','',text)\n#     text = re.sub(r'\\s\\s+',' ',text)\n#     text = ''.join(c for c in text if c <= '\\uFFFF')\n#     return text\ndef preprocess(file):\n  tokens = file.split()#Split by Whitespace\n  table = str.maketrans(' ', ' ', string.punctuation)#We can use the function maketrans() to create a mapping table\n  tokens = [w.translate(table) for w in tokens]#Python offers a function called translate() that will map one set of characters to another.\n  tokens = [word for word in tokens if word.isalpha()]#The isalpha() method returns True if all the characters are alphabet letters (a-z).\n  return tokens\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:58:07.741970Z","iopub.execute_input":"2024-01-15T17:58:07.742873Z","iopub.status.idle":"2024-01-15T17:58:18.263067Z","shell.execute_reply.started":"2024-01-15T17:58:07.742831Z","shell.execute_reply":"2024-01-15T17:58:18.262254Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(preprocess(\"In the text preprocessing function, the input text is first tokenized and lemmatized using spaCy to obtain a list of lemmatized tokens. This is followed by the removal of HTML entities, mentions (e.g., @username), URLs, and hashtags using regular expressions. Consecutive whitespaces are then reduced to a single space, and non-Unicode characters are removed, ensuring that the resulting text is clean and standardized. This comprehensive preprocessing pipeline aims to enhance the quality of the text data for downstream natural language processing tasks, such as sentiment analysis or text generation. Adjustments to the preprocessing steps can be made based on the specific requirements of the task at hand.\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:18:55.640591Z","iopub.execute_input":"2024-01-15T17:18:55.641475Z","iopub.status.idle":"2024-01-15T17:18:55.646924Z","shell.execute_reply.started":"2024-01-15T17:18:55.641440Z","shell.execute_reply":"2024-01-15T17:18:55.645773Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"['In', 'the', 'text', 'preprocessing', 'function', 'the', 'input', 'text', 'is', 'first', 'tokenized', 'and', 'lemmatized', 'using', 'spaCy', 'to', 'obtain', 'a', 'list', 'of', 'lemmatized', 'tokens', 'This', 'is', 'followed', 'by', 'the', 'removal', 'of', 'HTML', 'entities', 'mentions', 'eg', 'username', 'URLs', 'and', 'hashtags', 'using', 'regular', 'expressions', 'Consecutive', 'whitespaces', 'are', 'then', 'reduced', 'to', 'a', 'single', 'space', 'and', 'nonUnicode', 'characters', 'are', 'removed', 'ensuring', 'that', 'the', 'resulting', 'text', 'is', 'clean', 'and', 'standardized', 'This', 'comprehensive', 'preprocessing', 'pipeline', 'aims', 'to', 'enhance', 'the', 'quality', 'of', 'the', 'text', 'data', 'for', 'downstream', 'natural', 'language', 'processing', 'tasks', 'such', 'as', 'sentiment', 'analysis', 'or', 'text', 'generation', 'Adjustments', 'to', 'the', 'preprocessing', 'steps', 'can', 'be', 'made', 'based', 'on', 'the', 'specific', 'requirements', 'of', 'the', 'task', 'at', 'hand']\n","output_type":"stream"}]},{"cell_type":"code","source":"data=preprocess(soup.get_text())","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:58:19.984470Z","iopub.execute_input":"2024-01-15T17:58:19.985753Z","iopub.status.idle":"2024-01-15T17:58:20.630368Z","shell.execute_reply.started":"2024-01-15T17:58:19.985711Z","shell.execute_reply":"2024-01-15T17:58:20.629508Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(len(data))","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:25:03.969058Z","iopub.execute_input":"2024-01-15T17:25:03.969916Z","iopub.status.idle":"2024-01-15T17:25:03.974338Z","shell.execute_reply.started":"2024-01-15T17:25:03.969884Z","shell.execute_reply":"2024-01-15T17:25:03.973466Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"871786\n","output_type":"stream"}]},{"cell_type":"code","source":"len(set(data))#total","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:25:32.151984Z","iopub.execute_input":"2024-01-15T17:25:32.152833Z","iopub.status.idle":"2024-01-15T17:25:32.216369Z","shell.execute_reply.started":"2024-01-15T17:25:32.152798Z","shell.execute_reply":"2024-01-15T17:25:32.215285Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"33982"},"metadata":{}}]},{"cell_type":"code","source":"#here we predict the word 51th postion using 50th position\n#divide our data in chunks of 51 words and at the last we will separate the last word from every line\nlength_sentence = 50 + 1\nlines = []\nfor i in range(length_sentence, len(data)):\n  seq = data[i-length_sentence:i]\n  line = ' '.join(seq)\n  lines.append(line)\n  if i > 100000:#limit our dataset to 200000 words.\n    break\n\nprint(len(lines))","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:58:26.371615Z","iopub.execute_input":"2024-01-15T17:58:26.372006Z","iopub.status.idle":"2024-01-15T17:58:26.635451Z","shell.execute_reply.started":"2024-01-15T17:58:26.371974Z","shell.execute_reply":"2024-01-15T17:58:26.634466Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"99951\n","output_type":"stream"}]},{"cell_type":"code","source":"lines[0]","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:32:05.305347Z","iopub.execute_input":"2024-01-15T17:32:05.305950Z","iopub.status.idle":"2024-01-15T17:32:05.311753Z","shell.execute_reply.started":"2024-01-15T17:32:05.305917Z","shell.execute_reply":"2024-01-15T17:32:05.310666Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"'This is the Etext file presented by Project Gutenberg and is presented in cooperation with World Library Inc from their Library of the Future and Shakespeare CDROMS Project Gutenberg often releases Etexts that are NOT placed in the Public Domain Shakespeare This Etext has certain copyright implications you should read Project'"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\ntokenizer=Tokenizer()\ntokenizer.fit_on_texts(lines)\nsequence=tokenizer.texts_to_sequences(lines)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:58:29.634337Z","iopub.execute_input":"2024-01-15T17:58:29.635502Z","iopub.status.idle":"2024-01-15T17:58:37.994705Z","shell.execute_reply.started":"2024-01-15T17:58:29.635448Z","shell.execute_reply":"2024-01-15T17:58:37.993793Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"max_classes=len(tokenizer.word_index)+1\n","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:59:57.795697Z","iopub.execute_input":"2024-01-15T17:59:57.796787Z","iopub.status.idle":"2024-01-15T17:59:57.801930Z","shell.execute_reply.started":"2024-01-15T17:59:57.796735Z","shell.execute_reply":"2024-01-15T17:59:57.801024Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"sequences= np.array(sequence)\nx = sequences[:, :-1]\nyx = sequences[:, -1]\n\ny=to_categorical(yx,num_classes=max_classes)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:59:59.550779Z","iopub.execute_input":"2024-01-15T17:59:59.551703Z","iopub.status.idle":"2024-01-15T18:00:00.314027Z","shell.execute_reply.started":"2024-01-15T17:59:59.551665Z","shell.execute_reply":"2024-01-15T18:00:00.312912Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Assuming yx is your array of indices\nunique_values = np.unique(yx)\n\n# Check if the minimum value is 0 or 1\nif np.min(unique_values) == 0:\n    print(\"Indices are zero-based.\")\nelse:\n    print(\"Indices are one-based.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:07:14.815966Z","iopub.execute_input":"2024-01-15T17:07:14.816669Z","iopub.status.idle":"2024-01-15T17:07:14.983914Z","shell.execute_reply.started":"2024-01-15T17:07:14.816638Z","shell.execute_reply":"2024-01-15T17:07:14.982769Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Indices are zero-based.\n","output_type":"stream"}]},{"cell_type":"code","source":"seq_length = x.shape[1]\nseq_length","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:00:06.165675Z","iopub.execute_input":"2024-01-15T18:00:06.166087Z","iopub.status.idle":"2024-01-15T18:00:06.173374Z","shell.execute_reply.started":"2024-01-15T18:00:06.166050Z","shell.execute_reply":"2024-01-15T18:00:06.172379Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"50"},"metadata":{}}]},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1\nvocab_size\n\n\nyx","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:00:07.769315Z","iopub.execute_input":"2024-01-15T18:00:07.770268Z","iopub.status.idle":"2024-01-15T18:00:07.777489Z","shell.execute_reply.started":"2024-01-15T18:00:07.770232Z","shell.execute_reply":"2024-01-15T18:00:07.776263Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"array([616, 912,  11, ...,  12, 109,   5])"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\n# Example sequences\nsequences = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Separate input sequences (x) and target sequences (y)\nx = sequences[:, :-1]\ny = sequences[:, -1]\n\nprint(\"Input Sequences (x):\")\nprint(x)\nprint(\"\\nTarget Sequences (y):\")\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:00:41.320900Z","iopub.execute_input":"2024-01-13T19:00:41.321630Z","iopub.status.idle":"2024-01-13T19:00:41.328435Z","shell.execute_reply.started":"2024-01-13T19:00:41.321593Z","shell.execute_reply":"2024-01-13T19:00:41.327519Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stdout","text":"Input Sequences (x):\n[[1 2]\n [4 5]\n [7 8]]\n\nTarget Sequences (y):\n[3 6 9]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Shape of x:\", x.shape)\nseq_length= x.shape[1]\nprint(seq_length)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T17:49:48.139895Z","iopub.execute_input":"2024-01-15T17:49:48.140319Z","iopub.status.idle":"2024-01-15T17:49:48.146222Z","shell.execute_reply.started":"2024-01-15T17:49:48.140282Z","shell.execute_reply":"2024-01-15T17:49:48.144917Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Shape of x: (199951, 50)\n50\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.layers import LSTM,Dense,Embedding,Dropout\nfrom keras.models import Sequential\nmodel=Sequential()\nmodel.add(Embedding(max_classes,60,input_length=seq_length)) \nmodel.add(LSTM(units=32,return_sequences=True))\nmodel.add(LSTM(units=16))\n\n\n\n\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dense(max_classes, activation=\"softmax\"))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:13:36.076685Z","iopub.execute_input":"2024-01-15T18:13:36.077437Z","iopub.status.idle":"2024-01-15T18:13:36.615405Z","shell.execute_reply.started":"2024-01-15T18:13:36.077402Z","shell.execute_reply":"2024-01-15T18:13:36.614500Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Model: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_4 (Embedding)     (None, 50, 60)            548220    \n                                                                 \n lstm_6 (LSTM)               (None, 50, 32)            11904     \n                                                                 \n lstm_7 (LSTM)               (None, 16)                3136      \n                                                                 \n dense_8 (Dense)             (None, 16)                272       \n                                                                 \n dense_9 (Dense)             (None, 9137)              155329    \n                                                                 \n=================================================================\nTotal params: 718861 (2.74 MB)\nTrainable params: 718861 (2.74 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.optimizers import Adam\noptimizer = Adam(learning_rate=0.001)\nmodel.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\nmodel.fit(\n    x,\n    y,\n    batch_size=32,\n    epochs=20,\n    verbose=1\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:13:40.544992Z","iopub.execute_input":"2024-01-15T18:13:40.545388Z","iopub.status.idle":"2024-01-15T18:27:13.545951Z","shell.execute_reply.started":"2024-01-15T18:13:40.545357Z","shell.execute_reply":"2024-01-15T18:27:13.544957Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/20\n3124/3124 [==============================] - 65s 20ms/step - loss: 6.8524 - accuracy: 0.0288\nEpoch 2/20\n3124/3124 [==============================] - 41s 13ms/step - loss: 6.5368 - accuracy: 0.0367\nEpoch 3/20\n3124/3124 [==============================] - 40s 13ms/step - loss: 6.3637 - accuracy: 0.0459\nEpoch 4/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 6.2190 - accuracy: 0.0573\nEpoch 5/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 6.0858 - accuracy: 0.0638\nEpoch 6/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.9652 - accuracy: 0.0689\nEpoch 7/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.8626 - accuracy: 0.0772\nEpoch 8/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.7739 - accuracy: 0.0833\nEpoch 9/20\n3124/3124 [==============================] - 39s 13ms/step - loss: 5.6937 - accuracy: 0.0878\nEpoch 10/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.6205 - accuracy: 0.0917\nEpoch 11/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.5484 - accuracy: 0.0955\nEpoch 12/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.4804 - accuracy: 0.0994\nEpoch 13/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.4171 - accuracy: 0.1021\nEpoch 14/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.3616 - accuracy: 0.1045\nEpoch 15/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.3052 - accuracy: 0.1074\nEpoch 16/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.2528 - accuracy: 0.1107\nEpoch 17/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.2014 - accuracy: 0.1135\nEpoch 18/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.1536 - accuracy: 0.1165\nEpoch 19/20\n3124/3124 [==============================] - 39s 12ms/step - loss: 5.1080 - accuracy: 0.1189\nEpoch 20/20\n3124/3124 [==============================] - 38s 12ms/step - loss: 5.0649 - accuracy: 0.1215\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7ad5604105b0>"},"metadata":{}}]},{"cell_type":"code","source":"def generate_text(seed_text, model, tokenizer, max_sequence_length, num_words_to_generate):\n    generated_text = seed_text  # Initialize with the seed text\n    for _ in range(num_words_to_generate):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        while len(token_list) < max_sequence_length - 1:\n            token_list.insert(0, 0)  \n        token_list = token_list[-(max_sequence_length-1):]\n        input_sequence = np.array(token_list)\n        input_sequence = np.reshape(input_sequence, (1, -1))\n        predicted_probs = model.predict(input_sequence, verbose=0)\n        predicted_index = np.argmax(predicted_probs)\n        predicted_word = tokenizer.index_word.get(predicted_index, '')\n        seed_text += ' ' + predicted_word\n        generated_text += ' ' + predicted_word\n\n    return generated_text\n\n# Example Usage\ngenerated_text = generate_text(seed_text='''The''', model=model, tokenizer=tokenizer,\n                               max_sequence_length=seq_length+1, num_words_to_generate=100)\nprint(\"Generated Text:\", generated_text)\nprint(\"Generated Text:\", len(generated_text))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:33:50.271499Z","iopub.execute_input":"2024-01-15T18:33:50.271897Z","iopub.status.idle":"2024-01-15T18:33:55.758716Z","shell.execute_reply.started":"2024-01-15T18:33:50.271863Z","shell.execute_reply":"2024-01-15T18:33:55.757706Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Generated Text: The world and be done and we have done and we have been done and thou hast not be done and i have done and i have been done and i have been done and i have been done and i have been done and i have been done and i have been not not and be done and i have been done and i have been done and i have been not not and be done and i have been done and i have been not not and be done and i have been done and i have been not\nGenerated Text: 419\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_text(seed_text, model, tokenizer, max_sequence_length, num_words_to_generate):\n    generated_text = seed_text  # Initialize with the seed text\n    for _ in range(num_words_to_generate):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        while len(token_list) < max_sequence_length - 1:\n            token_list.insert(0, 0)  \n        token_list = token_list[-(max_sequence_length-1):]\n        input_sequence = np.array(token_list)\n        input_sequence = np.reshape(input_sequence, (1, -1))\n        predicted_probs = model.predict(input_sequence, verbose=0)\n        predicted_index = np.argmax(predicted_probs)\n        predicted_word = tokenizer.index_word.get(predicted_index, '')\n        seed_text += ' ' + predicted_word\n        generated_text += ' ' + predicted_word\n\n    return generated_text\n\n# Example Usage\ngenerated_text = generate_text(seed_text='''The Bird sang''', model=model, tokenizer=tokenizer,\n                               max_sequence_length=seq_length+1, num_words_to_generate=123)\nprint(\"Generated Text:\", generated_text)\nprint(\"Generated Text:\", len(generated_text))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-15T18:32:46.626435Z","iopub.execute_input":"2024-01-15T18:32:46.627135Z","iopub.status.idle":"2024-01-15T18:32:53.325851Z","shell.execute_reply.started":"2024-01-15T18:32:46.627089Z","shell.execute_reply":"2024-01-15T18:32:53.324696Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Generated Text: The Bird sang and i am not not not and be done and be done and we have been done and we have been done and thou hast not be done and i have done and i have been done and i have been done and i have been done and i have been done and i have been done and i have been not not and be done and i have been done and i have been done and i have been not not and be done and i have been done and i have been not not and be done and i have been done and i have been not not and be done and i have been done and i have been not\nGenerated Text: 518\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}