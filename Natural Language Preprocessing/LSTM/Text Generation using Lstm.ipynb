{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-13T18:26:04.986356Z","iopub.execute_input":"2024-01-13T18:26:04.987162Z","iopub.status.idle":"2024-01-13T18:26:04.993147Z","shell.execute_reply.started":"2024-01-13T18:26:04.987127Z","shell.execute_reply":"2024-01-13T18:26:04.992139Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow\nimport requests\nfrom bs4 import BeautifulSoup","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:14:49.403289Z","iopub.execute_input":"2024-01-13T19:14:49.403675Z","iopub.status.idle":"2024-01-13T19:14:49.408660Z","shell.execute_reply.started":"2024-01-13T19:14:49.403647Z","shell.execute_reply":"2024-01-13T19:14:49.407623Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"response=requests.get('https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt')\nsoup=BeautifulSoup(response.content,'html.parser')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:14:52.187431Z","iopub.execute_input":"2024-01-13T19:14:52.187820Z","iopub.status.idle":"2024-01-13T19:14:52.620530Z","shell.execute_reply.started":"2024-01-13T19:14:52.187787Z","shell.execute_reply":"2024-01-13T19:14:52.619531Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"code","source":"import spacy\nimport re,string\nfrom nltk.corpus import stopwords\nnlp = spacy.load('en_core_web_sm')\nnlp.max_length = 5281520\n\ndef preprocess(texts):\n    doc = nlp(texts)\n    text=\" \" .join([token.lemma_ for token in doc])\n    text=' '.join(word for word in text.split() if word.lower() not in stopwords.words('english'))\n    text = re.sub(r'\\&\\w*;','',text)\n    text = re.sub('@[^\\s]+','',text)\n    text = re.sub(r'\\$\\w*','',text)\n    text = text.lower()\n    text = re.sub(r'https?:\\/\\/.*\\/\\w*','',text)\n    text = re.sub(r'#\\w*','',text)\n    text = re.sub(r'[' + string.punctuation.replace('@','') + ']+',' ',text)\n    text = re.sub(r'\\b\\w{1,2}\\b','',text)\n    text = re.sub(r'\\s\\s+',' ',text)\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    text=re.sub(r'\\s+', ' ', text)\n    text = text.lstrip(' ')\n    text = ''.join(c for c in text if c <= '\\uFFFF')\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:21:32.485020Z","iopub.status.idle":"2024-01-13T19:21:32.485525Z","shell.execute_reply.started":"2024-01-13T19:21:32.485272Z","shell.execute_reply":"2024-01-13T19:21:32.485299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=preprocess(soup.get_text())","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:21:37.964261Z","iopub.execute_input":"2024-01-13T19:21:37.964630Z","iopub.status.idle":"2024-01-13T19:27:22.112024Z","shell.execute_reply.started":"2024-01-13T19:21:37.964600Z","shell.execute_reply":"2024-01-13T19:27:22.110928Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nvocab_size=len(tokenizer.word_index)+1\n\ntokenizer=Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(data)\nsequence=tokenizer.texts_to_sequences(data)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:27:34.588648Z","iopub.execute_input":"2024-01-13T19:27:34.589050Z","iopub.status.idle":"2024-01-13T19:28:19.927697Z","shell.execute_reply.started":"2024-01-13T19:27:34.589018Z","shell.execute_reply":"2024-01-13T19:28:19.926848Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"sequences_padded = pad_sequences(sequence,maxlen=30, padding='pre')\nsequences= np.array(sequences_padded)\nx=sequences[:,:-1]\nyx=sequences[:,-1]\nbatch_size=64\ny=to_categorical(yx,num_classes=vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:29:04.776500Z","iopub.execute_input":"2024-01-13T19:29:04.777102Z","iopub.status.idle":"2024-01-13T19:29:13.550044Z","shell.execute_reply.started":"2024-01-13T19:29:04.777068Z","shell.execute_reply":"2024-01-13T19:29:13.549011Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Example sequences\nsequences = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Separate input sequences (x) and target sequences (y)\nx = sequences[:, :-1]\ny = sequences[:, -1]\n\nprint(\"Input Sequences (x):\")\nprint(x)\nprint(\"\\nTarget Sequences (y):\")\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:00:41.320900Z","iopub.execute_input":"2024-01-13T19:00:41.321630Z","iopub.status.idle":"2024-01-13T19:00:41.328435Z","shell.execute_reply.started":"2024-01-13T19:00:41.321593Z","shell.execute_reply":"2024-01-13T19:00:41.327519Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stdout","text":"Input Sequences (x):\n[[1 2]\n [4 5]\n [7 8]]\n\nTarget Sequences (y):\n[3 6 9]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Shape of x:\", x.shape)\nseq_length= x.shape[1]","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:29:29.389773Z","iopub.execute_input":"2024-01-13T19:29:29.390542Z","iopub.status.idle":"2024-01-13T19:29:29.395686Z","shell.execute_reply.started":"2024-01-13T19:29:29.390508Z","shell.execute_reply":"2024-01-13T19:29:29.394641Z"},"trusted":true},"execution_count":180,"outputs":[{"name":"stdout","text":"Shape of x: (2834840, 29)\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.layers import LSTM,Dense,Embedding,Dropout\nfrom keras.models import Sequential\nmodel=Sequential()\nmodel.add(Embedding(vocab_size,50,input_length=seq_length)) \nmodel.add(LSTM(units=32,return_sequences=True))\n\nmodel.add(LSTM(units=16))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dense(16,activation='relu'))\n\nmodel.add(Dense(vocab_size, activation=\"softmax\"))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:29:55.307358Z","iopub.execute_input":"2024-01-13T19:29:55.308086Z","iopub.status.idle":"2024-01-13T19:29:55.855599Z","shell.execute_reply.started":"2024-01-13T19:29:55.308052Z","shell.execute_reply":"2024-01-13T19:29:55.854725Z"},"trusted":true},"execution_count":182,"outputs":[{"name":"stdout","text":"Model: \"sequential_23\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_20 (Embedding)    (None, 29, 50)            1900      \n                                                                 \n lstm_38 (LSTM)              (None, 29, 32)            10624     \n                                                                 \n lstm_39 (LSTM)              (None, 16)                3136      \n                                                                 \n dropout_17 (Dropout)        (None, 16)                0         \n                                                                 \n dense_51 (Dense)            (None, 16)                272       \n                                                                 \n dense_52 (Dense)            (None, 16)                272       \n                                                                 \n dense_53 (Dense)            (None, 38)                646       \n                                                                 \n=================================================================\nTotal params: 16850 (65.82 KB)\nTrainable params: 16850 (65.82 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.fit(\n    x,\n    y,\n    batch_size=batch_size,\n    epochs=3\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:29:58.239772Z","iopub.execute_input":"2024-01-13T19:29:58.240184Z","iopub.status.idle":"2024-01-13T19:45:39.982040Z","shell.execute_reply.started":"2024-01-13T19:29:58.240146Z","shell.execute_reply":"2024-01-13T19:45:39.980954Z"},"trusted":true},"execution_count":183,"outputs":[{"name":"stdout","text":"Epoch 1/3\n44295/44295 [==============================] - 317s 7ms/step - loss: 2.9041 - accuracy: 0.1599\nEpoch 2/3\n44295/44295 [==============================] - 312s 7ms/step - loss: 2.9019 - accuracy: 0.1600\nEpoch 3/3\n44295/44295 [==============================] - 311s 7ms/step - loss: 2.9018 - accuracy: 0.1600\n","output_type":"stream"},{"execution_count":183,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7b6135048b80>"},"metadata":{}}]},{"cell_type":"code","source":"def generate_text(seed_text, model, tokenizer, max_sequence_length, num_words_to_generate):\n    generated_text = seed_text  # Initialize with the seed text\n    for _ in range(num_words_to_generate):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        while len(token_list) < max_sequence_length - 1:\n            token_list.insert(0, 0)  \n        token_list = token_list[-(max_sequence_length-1):]\n        input_sequence = np.array(token_list)\n        input_sequence = np.reshape(input_sequence, (1, -1))\n        predicted_probs = model.predict(input_sequence, verbose=0)\n        predicted_index = np.argmax(predicted_probs)\n        predicted_word = tokenizer.index_word.get(predicted_index, '')\n        seed_text += ' ' + predicted_word\n        generated_text += ' ' + predicted_word\n\n    return generated_text\n\n# Example Usage\ngenerated_text = generate_text(seed_text='''hello ''', model=model, tokenizer=tokenizer,\n                               max_sequence_length=seq_length+1, num_words_to_generate=10)\nprint(\"Generated Text:\", generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:50:16.031340Z","iopub.execute_input":"2024-01-13T19:50:16.031715Z","iopub.status.idle":"2024-01-13T19:50:16.549190Z","shell.execute_reply.started":"2024-01-13T19:50:16.031679Z","shell.execute_reply":"2024-01-13T19:50:16.548136Z"},"trusted":true},"execution_count":192,"outputs":[{"name":"stdout","text":"Generated Text: hello           \n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_text(seed_text, model, tokenizer, max_sequence_length, num_words_to_generate):\n    generated_text = seed_text  # Initialize with the seed text\n    for _ in range(num_words_to_generate):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        while len(token_list) < max_sequence_length - 1:\n            token_list.insert(0, 0)  \n        token_list = token_list[-(max_sequence_length-1):]\n        input_sequence = np.array(token_list)\n        input_sequence = np.reshape(input_sequence, (1, -1))\n        predicted_probs = model.predict(input_sequence, verbose=0)\n        predicted_index = np.argmax(predicted_probs)\n        predicted_word = tokenizer.index_word.get(predicted_index, '')\n        seed_text += ' ' + predicted_word\n        generated_text += ' ' + predicted_word\n\n    return generated_text\n\n# Example Usage\ngenerated_text = generate_text(seed_text='''The Bird sang''', model=model, tokenizer=tokenizer,\n                               max_sequence_length=seq_length+1, num_words_to_generate=123)\nprint(\"Generated Text:\", generated_text)\nprint(\"Generated Text:\", len(generated_text))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-13T19:52:08.528029Z","iopub.execute_input":"2024-01-13T19:52:08.528422Z","iopub.status.idle":"2024-01-13T19:52:15.031044Z","shell.execute_reply.started":"2024-01-13T19:52:08.528391Z","shell.execute_reply":"2024-01-13T19:52:15.030090Z"},"trusted":true},"execution_count":197,"outputs":[{"name":"stdout","text":"Generated Text: The Bird sang                                                                                                                           \nGenerated Text: 136\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}